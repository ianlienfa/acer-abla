\begin{thebibliography}{5}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Xu et~al.(2021)Xu, Yang, Wang, and Liang]{xu2021doubly}
Tengyu Xu, Zhuoran Yang, Zhaoran Wang, and Yingbin Liang.
\newblock Doubly robust off-policy actor-critic: Convergence and optimality.
\newblock In \emph{International Conference on Machine Learning}, pages
  11581--11591. PMLR, 2021.

\bibitem[Su et~al.(2022)Su, Lee, Mulvey, and Poor]{9747378}
DiJia Su, Jason~D. Lee, John~M. Mulvey, and H.~Vincent Poor.
\newblock Competitive multi-agent reinforcement learning with self-supervised
  representation.
\newblock In \emph{ICASSP 2022 - 2022 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pages 4098--4102, 2022.
\newblock \doi{10.1109/ICASSP43922.2022.9747378}.

\bibitem[Lee and Moon(2021)]{lee2021deep}
Myoung~Hoon Lee and Jun Moon.
\newblock Deep reinforcement learning-based uav navigation and control: A soft
  actor-critic with hindsight experience replay approach.
\newblock \emph{arXiv preprint arXiv:2106.01016}, 2021.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1511.05952}, 2015.

\bibitem[Atkinson et~al.(2021)Atkinson, McCane, Szymanski, and
  Robins]{atkinson2021pseudo}
Craig Atkinson, Brendan McCane, Lech Szymanski, and Anthony Robins.
\newblock Pseudo-rehearsal: Achieving deep reinforcement learning without
  catastrophic forgetting.
\newblock \emph{Neurocomputing}, 428:\penalty0 291--307, 2021.

\end{thebibliography}
