\section{Detailed Implementation}
\label{section:implementation}
Please explain your implementation in detail. You may do this with the help of pseudo code or a figure of system architecture. Please also highlight which parts of the algorithm lead to the most difficulty in your implementation.


As for the part of ablations, we conduct the following alterations. 

\begin{enumerate}
    \item remove the entropy term
    \item remove \emph{trust region constraint}
    \item drop the bias term 
    \item remove the clamping on the actor loss
    \item change the \emph{truncation parameter}
    \item change $Q_opc$ to $Q_ret$ 
    \item change $Q_ret$ to $Q_opc$
    \item change the power term of truncated importance weight from $1/d$ to $1 / e^d$
    \item replace sdn structure to two independent networks
\end{enumerate}


1~5 are conducted under both discrete and continuous environments, while 6~9 are only considered under continuous environment.
\begin{enumerate}
    \item Remove the entropy term 
    As we can see, when we remove the entropy term, the performane will get worse. 
    \item Remove trust region constraint\\
    In the environment of CartPole and MountainCarContinuous, the removal of trust region constraint from the algorithm seems to be better, especially in the MountainCarContinuous environment. 
    \item Drop the bias term\\
    In both environment, the result with bias term dropped turns out to be significantly worse. 
    \item Remove the clamping on the actor loss\\
    In both environments, the result gets worse after we remove the clamping function. The reward would change slightly after certain episodes.
    \item Change the truncation parameter\\
    It appears that the results of each corresponding hyperparameters are almost the same in the last few episodes. 
    \item Change Qopc to Qret\\
    The result is worse after we conduct the state-action value replacement.
    \item Change Qret to Qopc\\
    The result is also worse after we conduct the state-action value replacement. Moreover, the reward would remain unchanged after some episodes.
    \item Change the power term of truncated importance weight from $1/d$ to $1/e^d$\\
    The change leads to a better performane, since d is the dimensionality of the action space. The alteration causes smaller truncated importane weight.
    \item Replace sdn structure to two independent networks\\
    The result gets worse after replacing the original network to two independent networks. The reward would change slightly after certain episodes.
\end{enumerate}
